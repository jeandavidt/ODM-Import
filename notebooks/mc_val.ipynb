{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from wbe_odm.odm_mappers import mcgill_mapper\n",
    "from config import *\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_lab = mcgill_mapper.McGillMapper()\n",
    "qc_lab.read(QC_VIRUS_DATA, STATIC_DATA, QC_VIRUS_SHEET_NAME, QC_VIRUS_LAB)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['QC_Compil_INSTITUTIONS (int)',\n 'QC_Compil_RESEAU (int)',\n 'QC_Compil_STEP (int)']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl = pd.ExcelFile(QC_VIRUS_DATA)\n",
    "val_sheets = [n for n in xl.sheet_names if 'Compil' in n and 'int' in n]\n",
    "val_sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QC_Compil_STEP (int)\n"
     ]
    }
   ],
   "source": [
    "sheet = val_sheets[-1]\n",
    "print(sheet)\n",
    "sheet_df = xl.parse(sheet, header=0, sorted=False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 15, 23]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(['Fin du Réseau - Intercepteur Est  (Site 1)',\n  'Fin du Réseau - Intercepteur Ouest  (Site 2)'],\n ['cp', 'cp'],\n ['01-Int_East', '02-Int_West'],\n ['qc_01', 'qc_02'],\n ['QC_01_cpFP24h_rawWW', 'QC_02_cpFP24h_rawWW'],\n [Timestamp('2021-05-16 00:00:00'), Timestamp('2021-05-16 00:00:00')],\n ['E', 'P'],\n ['O', 'X'])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet_cols = [str(col) for col in sheet_df.columns]\n",
    "pos_of_cols_w_headers = []\n",
    "idx_col_pos = 0\n",
    "\n",
    "for i, col in enumerate(sheet_cols):\n",
    "    if i==idx_col_pos:\n",
    "        continue\n",
    "    if 'Unnamed' not in col:\n",
    "        pos_of_cols_w_headers.append(i+1)\n",
    "\n",
    "last_sheet_col = len(sheet_cols)\n",
    "pos_of_cols_w_headers.append(last_sheet_col)\n",
    "print(pos_of_cols_w_headers)\n",
    "\n",
    "xl_start_cols = []\n",
    "xl_end_cols = []\n",
    "\n",
    "pos_of_last_item = len(pos_of_cols_w_headers) - 1\n",
    "for i in range(len(pos_of_cols_w_headers.copy())):\n",
    "    if i == pos_of_last_item:\n",
    "        #This is the end of the last df, so stop\n",
    "        break\n",
    "\n",
    "    start_pos = pos_of_cols_w_headers[i]\n",
    "    \n",
    "    if i == pos_of_last_item-1:\n",
    "        end_pos = pos_of_cols_w_headers[i+1]\n",
    "        \n",
    "    else:\n",
    "        end_pos = pos_of_cols_w_headers[i+1] -1\n",
    "\n",
    "\n",
    "    start_idx = mcgill_mapper.excel_style(start_pos+1)\n",
    "    end_idx = mcgill_mapper.excel_style(end_pos+1)\n",
    "    xl_start_cols.append(start_idx)\n",
    "    xl_end_cols.append(end_idx)\n",
    "\n",
    "sites_list = sheet_df.columns.to_list()\n",
    "sites_list = [i for i in sites_list if \"Unnamed\" not in i]\n",
    "\n",
    "type_codes = sheet_df.iloc[1].dropna().to_list()\n",
    "types = type_codes[::3]\n",
    "site_info = type_codes[1::3]\n",
    "label_ids = type_codes[2::3]\n",
    "last_dates = sheet_df.iloc[2].dropna().to_list()\n",
    "temp_dates =[]\n",
    "for item in last_dates:\n",
    "    try:\n",
    "        item = pd.to_datetime(item)\n",
    "        temp_dates.append(item)\n",
    "    except Exception:\n",
    "        continue\n",
    "last_dates = temp_dates\n",
    "sites = []\n",
    "for item in label_ids:\n",
    "    split = item.split(\"_\")[0:2]\n",
    "    site = \"_\".join(split).lower()\n",
    "    sites.append(site)\n",
    "sites_list, types, site_info, sites, label_ids, last_dates, xl_start_cols, xl_end_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names = sheet_df.iloc[3]\n",
    "cut_df = sheet_df.iloc[4:].copy()\n",
    "cut_df.columns = new_col_names\n",
    "cut_df.index = pd.to_datetime(cut_df.index)\n",
    "\n",
    "df_idx = 0\n",
    "reread_df = xl.parse(sheet, header=4, usecols=f\"{xl_start_cols[df_idx]}:{xl_end_cols[df_idx]}\")\n",
    "reread_idx = pd.to_datetime(xl.parse(sheet, header=4, usecols=\"A\", squeeze=True)).dt.strftime(\"%Y-%m-%d\")\n",
    "reread_df.set_index(reread_idx, inplace=True)\n",
    "reread_df.index = pd.to_datetime(reread_df.index)\n",
    "\n",
    "cols = reread_df.columns\n",
    "renamed_cols = {}\n",
    "for col in cols:\n",
    "    new_col = col\n",
    "    if re.match(\".*\\.[0-9]\", col):\n",
    "        if 'rejected' in col.lower():\n",
    "            continue\n",
    "        dot_idx = col.find(\".\")\n",
    "        new_col = col[0:dot_idx]\n",
    "    \n",
    "    renamed_cols[col] = new_col\n",
    "reread_df.rename(columns = renamed_cols, inplace=True) \n",
    "\n",
    "cols_to_keep = [\"BRSV (%rec)\",\"Rejected by\", \"PMMV (gc/ml)\",\"Rejected by.1\", \"SARS (gc/ml)\", \"Rejected by.2\", \"Quality Note\"]\n",
    "reread_df = reread_df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = qc_lab.sample\n",
    "ww = qc_lab.ww_measure\n",
    "\n",
    "charac = {\n",
    "    \"BRSV (%rec)\": {\n",
    "        \"unit\": \"pctRecovery\",\n",
    "        \"type\": \"nBRSV\",\n",
    "    },\n",
    "    \"PMMV (gc/ml)\": {\n",
    "        \"unit\": \"gc/ml\",\n",
    "        \"type\": \"nPMMoV\",\n",
    "    },\n",
    "    \"SARS (gc/ml)\": {\n",
    "        \"unit\": \"gc/ml\",\n",
    "        \"type\": \"covN2\",\n",
    "    }\n",
    "}\n",
    "copied = reread_df.copy()\n",
    "\n",
    "# Prepping date columns\n",
    "samples[\"dateTimeEnd\"] = pd.to_datetime(samples[\"dateTimeEnd\"]) \n",
    "samples[\"dateTime\"] = pd.to_datetime(samples[\"dateTime\"]) \n",
    "\n",
    "sample_type_filt = samples[\"type\"].str.contains(types[df_idx])\n",
    "sample_sites_filt = samples[\"siteID\"].str.lower().str.contains(sites[df_idx])\n",
    "for i, row in copied.iterrows():\n",
    "    sample_date_filt1 = samples[\"dateTimeEnd\"].dt.date == pd.to_datetime(row.name).date()\n",
    "    sample_date_filt2 = samples[\"dateTime\"].dt.date == pd.to_datetime(row.name).date()\n",
    "    sample_date_filt = sample_date_filt1 | sample_date_filt2\n",
    "    sample_tot_filt = sample_date_filt & sample_sites_filt & sample_type_filt\n",
    "\n",
    "    samples.loc[sample_tot_filt, [\"qualityFlag\", \"notes\"]] = [True, row[\"Quality Note\"]]\n",
    "\n",
    "    sample_list = samples.loc[sample_tot_filt, \"sampleID\"].drop_duplicates().to_list()\n",
    "\n",
    "    for col, wwm_info in charac.items():\n",
    "        ww_type_filt = ww[\"type\"].str.lower().str.contains(wwm_info[\"type\"])\n",
    "        ww_unit_filt = ww[\"unit\"].str.lower().str.contains(wwm_info[\"unit\"])\n",
    "        ww_sample_filt = ww[\"sampleID\"].isin(sample_list)\n",
    "        ww_tot_filt = ww_type_filt & ww_unit_filt & ww_sample_filt\n",
    "    \n",
    "        ww.loc[ww_tot_filt, [\"qualityFlag\", \"notes\"]] = [True, row[\"Quality Note\"]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "757       wqph\n758       wqph\n759       wqph\n760       wqph\n761       wqph\n         ...  \n12086    covn2\n12087    covn2\n12088    covn2\n12089    covn2\n12090    covn2\nName: type, Length: 6439, dtype: object"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww[\"type\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_excel() got an unexpected keyword argument 'sorted'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bfaa107ccd24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m \u001b[0mread_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqc_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQC_VIRUS_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"QC_Compil_STEP (int)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-bfaa107ccd24>\u001b[0m in \u001b[0;36mread_validation\u001b[0;34m(mapper, path, sheet_name)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0msheet_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mlast_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_last_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-bfaa107ccd24>\u001b[0m in \u001b[0;36mextract_dfs\u001b[0;34m(path, sheet_name, idx_col_pos, header_row_pos)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_col_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_row_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0msheet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0msheet_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msheet_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mstart_borders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_borders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_df_borders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/covid/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: read_excel() got an unexpected keyword argument 'sorted'"
     ]
    }
   ],
   "source": [
    "sheetname = \"QC_COMPIL_STEP (int)\"\n",
    "\n",
    "def find_df_borders(sheet_cols):\n",
    "    for i, col in enumerate(sheet_cols):\n",
    "        if i==idx_col_pos:\n",
    "            continue\n",
    "        if 'Unnamed' not in col:\n",
    "            pos_of_cols_w_headers.append(i+1)\n",
    "    last_sheet_col = len(sheet_cols)\n",
    "    pos_of_cols_w_headers.append(last_sheet_col)\n",
    "    print(pos_of_cols_w_headers)\n",
    "\n",
    "    xl_start_cols = []\n",
    "    xl_end_cols = []\n",
    "\n",
    "    pos_of_last_item = len(pos_of_cols_w_headers) - 1\n",
    "    for i in range(len(pos_of_cols_w_headers.copy())):\n",
    "        if i == pos_of_last_item:\n",
    "            #This is the end of the last df, so stop\n",
    "            break\n",
    "\n",
    "        start_pos = pos_of_cols_w_headers[i]\n",
    "        \n",
    "        if i == pos_of_last_item-1:\n",
    "            end_pos = pos_of_cols_w_headers[i+1]\n",
    "            \n",
    "        else:\n",
    "            end_pos = pos_of_cols_w_headers[i+1] -1\n",
    "\n",
    "\n",
    "        start_idx = mcgill_mapper.excel_style(start_pos+1)\n",
    "        end_idx = mcgill_mapper.excel_style(end_pos+1)\n",
    "        xl_start_cols.append(start_idx)\n",
    "        xl_end_cols.append(end_idx)\n",
    "    return xl_start_cols, xl_end_cols\n",
    "\n",
    "\n",
    "def get_type_codes(sheet_df):\n",
    "    return sheet_df.iloc[1].dropna().to_list()\n",
    "\n",
    "\n",
    "def get_sample_type(type_codes):\n",
    "    return type_codes[::3]\n",
    "\n",
    "\n",
    "def get_last_dates(sheet_df):\n",
    "    dates = sheet_df.iloc[2].dropna().to_list()\n",
    "    temp_dates =[]\n",
    "    for item in dates:\n",
    "        try:\n",
    "            item = pd.to_datetime(item)\n",
    "            temp_dates.append(item)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return temp_dates\n",
    "\n",
    "\n",
    "def get_label_ids(type_codes):\n",
    "    return type_codes[2::3]\n",
    "\n",
    "\n",
    "def get_site_ids(label_ids):\n",
    "    sites = []\n",
    "    for item in label_ids:\n",
    "        split = item.split(\"_\")[0:2]\n",
    "        site = \"_\".join(split).lower()\n",
    "        sites.append(site)\n",
    "    return sites\n",
    "\n",
    "def get_values_df(path, sheet_name, start, end, header_row_pos):\n",
    "    return pd.read_excel(\n",
    "            path,\n",
    "            sheet_name=sheet_name,\n",
    "            header=header_row_pos,\n",
    "            usecols =f\"{start}:{end}\"\n",
    "            )\n",
    "\n",
    "def get_index_series(path, sheet_name, idx_col):\n",
    "    idx_series = pd.read_excel(\n",
    "            path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols = idx_col,\n",
    "            squeeze=True\n",
    "        )\n",
    "    idx_series = pd.to_datetime(idx_series).dt.strftime(\"%Y-%m-%d\")\n",
    "    return pd.to_datetime(idx_series)\n",
    "    \n",
    "def clean_names(df):\n",
    "    cols = df.columns\n",
    "    renamed_cols = {}\n",
    "    for col in cols:\n",
    "        new_col = col\n",
    "        if re.match(\".*\\.[0-9]\", col):\n",
    "            if 'rejected' in col.lower():\n",
    "                continue\n",
    "            dot_idx = col.find(\".\")\n",
    "            new_col = col[0:dot_idx]\n",
    "    \n",
    "    renamed_cols[col] = new_col\n",
    "    return df.rename(columns = renamed_cols) \n",
    "\n",
    "def extract_dfs(path, sheet_name, idx_col_pos=0, header_row_pos=4):\n",
    "    sheet_df = pd.read_excel(path, sheet_name=sheet_name, header=0, sorted=False, index_col=0)\n",
    "    sheet_cols = [str(col) for col in sheet_df.columns]\n",
    "    start_borders, end_borders = find_df_borders(sheet_cols)\n",
    "    idx_col = mcgill_mapper.excel_style(idx_col_pos+1)\n",
    "    \n",
    "    dfs = []\n",
    "    for start, end in zip(start_borders, end_borders):\n",
    "        vals = get_values_df(path, sheet_name, start, end, header_row_pos)\n",
    "        idx = get_index_series(path, sheet_name, idx_col)\n",
    "        df = vals.set_index(idx)\n",
    "        df = clean_names(df)\n",
    "        cols_to_keep = [\"BRSV (%rec)\",\"Rejected by\", \"PMMV (gc/ml)\",\"Rejected by.1\", \"SARS (gc/ml)\", \"Rejected by.2\", \"Quality Note\"]\n",
    "        df = df[cols_to_keep]\n",
    "        dfs.append(df)\n",
    "    return sheet_df, dfs\n",
    "\n",
    "\n",
    "def parse_dates(df):\n",
    "    for col in df.columns:\n",
    "        if 'dateTime' in col:\n",
    "            df[col] = pd.to_datetime(col)\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_quality_checks(mapper,  v_df, last_date, site_id, sample_type):\n",
    "    charac = {\n",
    "        \"BRSV (%rec)\": {\n",
    "            \"unit\": \"pctRecovery\",\n",
    "            \"type\": \"nBRSV\",\n",
    "        },\n",
    "        \"PMMV (gc/ml)\": {\n",
    "            \"unit\": \"gc/ml\",\n",
    "            \"type\": \"nPMMoV\",\n",
    "        },\n",
    "        \"SARS (gc/ml)\": {\n",
    "            \"unit\": \"gc/ml\",\n",
    "            \"type\": \"covN2\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    samples = mapper.sample\n",
    "    samples = parse_dates(samples)\n",
    "    ww = mapper.ww_measure\n",
    "    \n",
    "    sample_type_filt = samples[\"type\"].str.contains(sample_type)\n",
    "    sample_sites_filt = samples[\"siteID\"].str.lower().str.contains(site_id)\n",
    "    for _, row in v_df.iterrows():\n",
    "        sample_date_filt1 = samples[\"dateTimeEnd\"].dt.date == pd.to_datetime(row.name).date()\n",
    "        sample_date_filt2 = samples[\"dateTime\"].dt.date == pd.to_datetime(row.name).date()\n",
    "        sample_date_filt = sample_date_filt1 | sample_date_filt2\n",
    "        sample_tot_filt = sample_date_filt & sample_sites_filt & sample_type_filt\n",
    "\n",
    "        samples.loc[sample_tot_filt, [\"qualityFlag\", \"notes\"]] = [True, row[\"Quality Note\"]]\n",
    "\n",
    "        sample_list = samples.loc[sample_tot_filt, \"sampleID\"].drop_duplicates().to_list()\n",
    "\n",
    "        for col, wwm_info in charac.items():\n",
    "            ww_type_filt = ww[\"type\"].str.lower().str.contains(wwm_info[\"type\"])\n",
    "            ww_unit_filt = ww[\"unit\"].str.lower().str.contains(wwm_info[\"unit\"])\n",
    "            ww_sample_filt = ww[\"sampleID\"].isin(sample_list)\n",
    "            ww_tot_filt = ww_type_filt & ww_unit_filt & ww_sample_filt\n",
    "        \n",
    "            ww.loc[ww_tot_filt, [\"qualityFlag\", \"notes\"]] = [True, row[\"Quality Note\"]]\n",
    "\n",
    "    if 'grb' in sample_type:\n",
    "        sample_last_date_filt = samples.loc[samples['dateTime'] > last_date]\n",
    "    else:\n",
    "        sample_last_date_filt = samples.loc[samples['dateTimeEnd'] > last_date]\n",
    "    \n",
    "    unchecked_filt = sample_type_filt & sample_sites_filt & sample_last_date_filt\n",
    "    samples.loc[unchecked_filt, [\"qualityFlag\", \"notes\"]] = [True, \"Unchecked\"]\n",
    "    \n",
    "    unchecked_sample_ids = samples.loc[unchecked_filt, \"sampleID\"].drop_duplicates().to_list()\n",
    "\n",
    "    ww.loc[ww['sampleID'].isin(unchecked_sample_ids), [\"qualityFlag\", \"notes\"]] = [True, \"Unchecked\"]\n",
    "    \n",
    "    mapper.sample = samples\n",
    "    mapper.ww_measure = ww\n",
    "    return mapper\n",
    "\n",
    "def read_validation(mapper, path, sheet_name):\n",
    "    sheet_df, dfs = extract_dfs(path, sheet_name)\n",
    "\n",
    "    last_dates = get_last_dates(sheet_df)\n",
    "    type_codes = get_type_codes(sheet_df)\n",
    "    sample_types = get_sample_type(sheet_df)\n",
    "    label_ids = get_label_ids(type_codes)\n",
    "    site_ids = get_site_ids(label_ids)\n",
    "\n",
    "    for v_df, last_date, site_id, sample_type in zip(dfs, last_dates, site_ids, sample_types):\n",
    "        mapper = apply_quality_checks(mapper, v_df, last_date, site_id, sample_type)\n",
    "    return mapper\n",
    "\n",
    "read_validation(qc_lab, QC_VIRUS_DATA, \"QC_Compil_STEP (int)\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fff2645691be52a4acf54b6569930c06fb1b99dce13069d7478dfbb615d05851"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('covid': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}